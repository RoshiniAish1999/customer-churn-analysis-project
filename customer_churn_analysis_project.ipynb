{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "customer- churn -analysis- project.ipynb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RoshiniAish1999/customer-churn-analysis-project/blob/main/customer_churn_analysis_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Problem Definition**"
      ],
      "metadata": {
        "id": "7_hJvZ9d8si8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To predict customer churn for a subscription service\n",
        "Churn refers to customers leaving the service.\n",
        "\n",
        "Why is this important? For example:\n",
        "- Churn prediction helps businesses retain customers.\n",
        "- Reducing churn saves costs on acquiring new customers."
      ],
      "metadata": {
        "id": "a2P_K7j59Kh2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##DATA COLLECTION AND EXPLORATION"
      ],
      "metadata": {
        "id": "PVilGeTKAeOi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXKNJIzJ1tPN"
      },
      "outputs": [],
      "source": [
        "import numpy as np #\n",
        "import pandas as pd #\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv(\"/content/Churn_Modelling.csv\")"
      ],
      "metadata": {
        "id": "5c8f3LHx17_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "wkGP9Mff2Ehe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe().T.round(2)"
      ],
      "metadata": {
        "id": "oTXNUeRF2G_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "4VZ5mzK-2JLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "bXmdr7ly2Lmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missing_df =  df.isnull().sum().to_frame().rename(columns={0:\"Total No. of Missing Values\"})\n",
        "missing_df[\"% of Missing Values\"] = round((missing_df[\"Total No. of Missing Values\"]/len( df))*100,2)\n",
        "missing_df"
      ],
      "metadata": {
        "id": "rI9LPe0A2Nr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop([\"RowNumber\", \"CustomerId\", \"Surname\"], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "vyfPKBPW2RYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "MQTfTG722Uo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "62ZvnULA2WuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploratory Data Analysis (EDA)"
      ],
      "metadata": {
        "id": "FNHKo7im-YtR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"CreditCard\"] = df[\"HasCrCard\"].apply(lambda x: \"credit card present\" if x == 1 else \"no credit card\")\n",
        "df[\"IsActive\"] = df[\"IsActiveMember\"].apply(lambda x: \"active\" if x == 1 else \"not active\")\n",
        "df[\"outcome\"] = df[\"Exited\"].apply(lambda x: \"quit\" if x == 1 else \"did not quit\")\n",
        "\n",
        "##code is transforming numerical columns into more readable categorical labels using the .apply(lambda x: ...) function"
      ],
      "metadata": {
        "id": "CK60YYWg2YsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"gender_quit\"] = df[\"Gender\"] + '-' + df[\"outcome\"]\n",
        "df[\"geography_quit\"] = df[\"Geography\"] + '-' + df[\"outcome\"]\n",
        "df[\"card_quit\"] = df[\"CreditCard\"] + '-' + df[\"outcome\"]\n",
        "df[\"active_quit\"] = df[\"IsActive\"] + '-' + df[\"outcome\"]\n",
        "\n",
        "##code is creating new categorical features by combining different columns. This helps in feature engineering, allowing models to detect relationships between variables more effectively."
      ],
      "metadata": {
        "id": "TB9m9oW82a3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pies = [\"Geography\", \"Gender\", \"CreditCard\", \"IsActive\", \"outcome\"]\n",
        "fig, axes = plt.subplots(nrows=1, ncols=5, figsize=(16, 8))\n",
        "for i in range(len(pies)):\n",
        "    counts = df[pies[i]].value_counts()\n",
        "    axes[i].pie(counts, autopct=\"%0.2f%%\", labels=counts)\n",
        "    axes[i].legend(counts.index)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "05WBoxmt2dV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "churn_dist = df['Exited'].value_counts(normalize=True) * 100\n",
        "churn_dist.plot(kind='bar', color=['green', 'red'])\n",
        "plt.title(\"Churn Distribution\")\n",
        "plt.ylabel(\"Percentage\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "gLPmkrM258K4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select only numerical features for correlation calculation\n",
        "numerical_df = df.select_dtypes(include=np.number)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(numerical_df.corr(), annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n",
        "plt.title(\"Correlation Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WmlWsRi56GCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(data=df, x='Gender', hue='Exited')\n",
        "plt.title('Churn by Gender')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "zJCw9JFz6OF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MODEL BUILDING\n"
      ],
      "metadata": {
        "id": "bmlhzC99_8Cc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df[['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'EstimatedSalary']] #select relevant features for training, it can also be all numerical features like done in numerical_df\n",
        "y = df['Exited']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # Adjust test_size and random_state as needed\n"
      ],
      "metadata": {
        "id": "oqPDmYwa_AoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset was split into features (X) and target (y) variables.\n",
        "Features: All columns except for identifiers and the target variable (Exited).\n",
        "Target: The Exited column, which indicates churn (1 = Quit, 0 = Did Not Quit).\n",
        "The dataset was split into:\n",
        "Training set (70%): Used to train the machine learning models.\n",
        "Testing set (30%): Used to evaluate model performance.\n",
        "A random_state ensures reproducibility."
      ],
      "metadata": {
        "id": "25embojA_rcE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LOGISTIC REGRESSION"
      ],
      "metadata": {
        "id": "N-QnYDE4BSrI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: generate logistic regression\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Initialize and train the Logistic Regression model\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_log_reg = log_reg.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_log_reg = accuracy_score(y_test, y_pred_log_reg)\n",
        "print(f\"Logistic Regression Accuracy: {accuracy_log_reg}\")\n",
        "print(classification_report(y_test, y_pred_log_reg))\n",
        "print(confusion_matrix(y_test, y_pred_log_reg))\n"
      ],
      "metadata": {
        "id": "_nGm1vTAKs4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overview: Logistic Regression is a linear model that predicts the probability of an event occurring (in this case, customer churn). It assumes a linear relationship between the independent variables (features) and the log-odds of the dependent variable (target).\n",
        "\n",
        "Why Use Logistic Regression?\n",
        "\n",
        "Simplicity: Itâ€™s easy to implement and interpret, making it suitable for smaller datasets or when the relationships between variables are straightforward.\n",
        "Linear Separability: Works well if the dataset has a clear boundary between churned and non-churned customers in terms of feature values.\n",
        "Probability Outputs: Logistic Regression outputs a probability (0 to 1), which is useful for ranking and thresholding decisions.\n",
        "\n",
        "Performance:\n",
        "Accuracy: 80%\n",
        "Logistic Regression provides a good baseline model but may not perform well if the dataset contains non-linear relationships or complex feature interactions.\n",
        "\n"
      ],
      "metadata": {
        "id": "0a0XdSch_u_N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RANDOM FOREST"
      ],
      "metadata": {
        "id": "grIgjUA7Buag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: generate random forest\n",
        "\n",
        "# Initialize and train the Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42) #n_estimators is the number of trees in the forest\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_rf = rf_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "print(f\"Random Forest Accuracy: {accuracy_rf}\")\n",
        "print(classification_report(y_test, y_pred_rf))\n",
        "print(confusion_matrix(y_test, y_pred_rf))\n"
      ],
      "metadata": {
        "id": "f-he2QKYK2ei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overview: Random Forest is an ensemble learning method that combines multiple decision trees to make predictions. It aggregates the results of individual trees (via majority voting for classification or averaging for regression) to improve accuracy and reduce overfitting.\n",
        "\n",
        "Why Use Random Forest?\n",
        "\n",
        "Handles Non-Linearity: Captures complex and non-linear relationships in the data that simpler models like Logistic Regression cannot handle.\n",
        "Reduces Overfitting: By averaging multiple trees, Random Forest minimizes the risk of overfitting, which is common with single decision trees.\n",
        "Feature Importance: Random Forest provides insights into the relative importance of features, helping identify key drivers of churn.\n",
        "Robustness: Handles missing data and noisy datasets effectively.\n",
        "\n",
        "Performance:\n",
        "Accuracy: 84%\n",
        "The higher accuracy compared to Logistic Regression suggests that Random Forest is better suited for this dataset due to its ability to model complex feature interactions.\n",
        "Feature Insights: Feature importance can be extracted using rf_model.feature_importances_ to analyze which variables contribute most to churn predictions."
      ],
      "metadata": {
        "id": "F7EA0IdiB6vE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performance Metrics"
      ],
      "metadata": {
        "id": "loJF8ov2CUlH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models = [\"Logistic Regression\", \"Random Forest\"]\n",
        "accuracies = [accuracy_score(y_test, y_pred_lr), accuracy_score(y_test, y_pred_rf)]\n",
        "plt.bar(models, accuracies, color=['blue', 'orange'])\n",
        "plt.title(\"Model Accuracy Comparison\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Toktc_176quw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy tells us the overall performance of the model in predicting customer churn (whether a customer quits or not).\n",
        "For example, if the Random Forest model achieves 84% accuracy, this means 84 out of 100 customers are correctly classified as churners or non-churners.\n",
        "Business Impact: While accuracy is important, it doesn't capture the cost of misclassifying churners (False Negatives) or overestimating churn (False Positives), which can lead to missed revenue opportunities or unnecessary retention efforts.\n"
      ],
      "metadata": {
        "id": "cXehhvD_DMwo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "importances = rf_model.feature_importances_\n",
        "features = X_train.columns\n",
        "sns.barplot(x=importances, y=features)\n",
        "plt.title(\"Feature Importance\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "P_rplv_K6vHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "y_probs = rf_model.predict_proba(X_test)[:, 1]\n",
        "fpr, tpr, _ = roc_curve(y_test, y_probs)\n",
        "plt.plot(fpr, tpr, label=f\"AUC = {auc(fpr, tpr):.2f}\")\n",
        "plt.plot([0, 1], [0, 1], 'r--')\n",
        "plt.title(\"ROC Curve\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TiAMOJ3B7KAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ROC curve visualizes the trade-off between True Positive Rate (Recall) and False Positive Rate.\n",
        "The AUC score quantifies this trade-off: an AUC of 0.85 for Random Forest indicates that the model is highly effective at distinguishing between churners and non-churners.\n",
        "Business Impact:\n",
        "A high AUC score ensures confidence in the model's ability to rank churners higher than non-churners, improving the prioritization of customers for retention campaigns.\n"
      ],
      "metadata": {
        "id": "R8f0bsQWDmej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot(cmap=\"Blues\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Uq3D8lJKMrbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## INSIGHT USING METRICS\n",
        "\n",
        "Logistic Regression:\n",
        "Accuracy: 80%\n",
        "Precision: High for non-churners but moderate for churners.\n",
        "Recall: Moderate; some churners are missed.\n",
        "Implication: Logistic Regression is better for baseline understanding but might miss complex patterns in data.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Random Forest:\n",
        "Accuracy: 84%\n",
        "Precision: More balanced for churners and non-churners.\n",
        "Recall: Higher, capturing more churners.\n",
        "AUC: 0.85, suggesting stronger differentiation capability.\n",
        "Implication: Random Forest is better suited for this project due to its ability to model non-linear relationships and interactions, improving recall and precision.\n"
      ],
      "metadata": {
        "id": "bcYYfaf0EZkP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RECOMMENDATIONS\n",
        "\n",
        "\n",
        "1.Targeted Promotions & Loyalty Programs\n",
        "Why? Customers with a low balance or inactive accounts have a higher likelihood of churning.\n",
        "Action Plan:\n",
        "Offer personalized discounts or cashback offers to encourage engagement.\n",
        "Implement tiered loyalty programs rewarding long-term customers.\n",
        "Introduce time-sensitive promotional offers to reactivate dormant users.\n",
        "\n",
        "\n",
        "2. Segment-Specific Engagement Strategies\n",
        "Why? Certain customer demographics (e.g., specific gender, age group, or geographic region) may have a higher churn rate.\n",
        "Action Plan:\n",
        "Conduct customer surveys to understand pain points in at-risk regions.\n",
        "Develop region-specific customer support initiatives.\n",
        "Offer incentives for referrals and word-of-mouth marketing to retain customers within high-churn segments.\n",
        "\n",
        "3. Personalized Customer Experience & Support\n",
        "Why? Poor service, unresolved complaints, or lack of engagement contribute to customer churn.\n",
        "Action Plan:\n",
        "Implement AI-driven chatbots and proactive customer support to address issues before they escalate.\n",
        "Provide dedicated account managers for high-value customers to ensure personalized attention.\n",
        "Improve onboarding processes to make new customers feel valued and comfortable with the services.\n",
        "\n",
        "\n",
        "4. Proactive Retention Strategies for High-Risk Customers\n",
        "Why? Customers predicted to churn should be engaged before they decide to leave.\n",
        "Action Plan:\n",
        "Create an early warning system using predictive analytics to flag at-risk customers.\n",
        "Implement retention call campaigns where customer service representatives reach out to high-risk customers.\n",
        "Offer flexible payment options or account management features to customers showing signs of disengagement.\n",
        "\n",
        "\n",
        "5. Data-Driven Decision Making\n",
        "Why? The model provides key insights into the factors that influence churn, which should guide business strategies.\n",
        "Action Plan:\n",
        "Continuously monitor customer data and update models to improve prediction accuracy.\n",
        "A/B test different retention strategies and measure effectiveness using KPIs.\n",
        "Integrate predictive analytics into the CRM system to automate customer segmentation and outreach.\n",
        "\n",
        "\n",
        "6. Product & Service Enhancements\n",
        "Why? Product dissatisfaction is a major reason for customer churn.\n",
        "Action Plan:\n",
        "Analyze feedback from churned customers to improve products and services.\n",
        "Introduce innovative features that align with customer needs.\n",
        "Ensure a seamless user experience through mobile-friendly services and intuitive interfaces."
      ],
      "metadata": {
        "id": "UZCdSR8BFSrU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7zQxkZYvEe4r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}